# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from config import settings
from services.user_service import get_user_info, get_daily_activity
from services.mongo_service import save_chat, get_user_chats
import google.generativeai as genai

# MODEL_NAME = settings.KANANA_MODEL
# TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME, token=settings.HF_TOKEN)
# MODEL = AutoModelForCausalLM.from_pretrained(
#     MODEL_NAME,
#     torch_dtype="auto",
#     device_map="auto",
#     token=settings.HF_TOKEN,
# )
# generator = pipeline("text-generation", model=MODEL, tokenizer=TOKENIZER, device_map="auto")

genai.configure(api_key=settings.GOOGLE_API_KEY)
model = genai.GenerativeModel("gemini-2.5-flash")

# ìœ í‹¸: ë¹ˆ ê°’ ì œê±°
def _fmt(label: str, value, unit: str = ""):
    if not value or value in ["N/A", "None", "ë¹„ê³µê°œ"]:
        return ""
    return f"- {label}: {value}{unit}"

#ì¼ë°˜ ìˆ˜ë©´ ëŒ€í™”ìš©
def generate_general_chat(req):
    """ì¼ìƒ ìˆ˜ë©´ ê´€ë ¨ ëŒ€í™” (í”¼ë¡œë„ ë°ì´í„° ì—†ì´ ì¼ë°˜ ì§ˆë¬¸ ëŒ€ì‘)"""
    prompt = f"""
    ë„ˆëŠ” ìˆ˜ë©´ ì½”ì¹˜ì§€ë§Œ, ì¹œêµ¬ì²˜ëŸ¼ í¸í•˜ê²Œ ëŒ€í™”í•´ì£¼ëŠ” ì±—ë´‡ì´ì•¼.
    ì‚¬ìš©ìê°€ ìˆ˜ë©´, ìŠ¤íŠ¸ë ˆìŠ¤, í”¼ë¡œ, ë£¨í‹´, íœ´ì‹ ë“±ì— ëŒ€í•´ ë¬¼ì–´ë³´ë©´
    ë”°ëœ»í•˜ê³  ê³µê°ê°€ëŠ” ë§íˆ¬ë¡œ ëŒ€ë‹µí•´ì¤˜.

    ë‹¨, ë„ˆë¬´ ê³¼í•™ì ì´ê±°ë‚˜ ì˜í•™ì ì¸ ì„¤ëª…ì€ í”¼í•˜ê³ ,
    ì§§ê³  ë¶€ë“œëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì´ì•¼ê¸°í•˜ë“¯ ë‹µí•´ì¤˜ ğŸ˜Š

    ì‚¬ìš©ì ì§ˆë¬¸: {req.message}
    """

    result = model.generate_content(prompt)
    response = result.text.strip()

    save_chat(req.user_id, req.message, response, chat_type="general")
    return response

# ì¼ê°„ ë¦¬í¬íŠ¸ (í•˜ë£¨ ë°ì´í„° ê¸°ë°˜)
def generate_daily_report(user_id:int):
    user = get_user_info(user_id) or {}
    activity = get_daily_activity(user_id) or {}
    
    if not activity:
        return "ì˜¤ëŠ˜ì˜ ìˆ˜ë©´ ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤"
    
    prompt = f"""
    ë„ˆëŠ” ì „ë¬¸ì ì¸ ìˆ˜ë©´ ì½”ì¹˜ì•¼ 
    ì•„ë˜ì˜ ì‚¬ìš©ì í•˜ë£¨ ìˆ˜ë©´ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì„œ 'ì˜¤ëŠ˜ì˜ ìˆ˜ë©´ ë¦¬í¬íŠ¸'ë¥¼ ì‘ì„±í•´ì¤˜.
    
    [ì‚¬ìš©ì]
    ì´ë¦„: {user.get('name', 'ë¹„ê³µê°œ')}
    ë‚˜ì´: {user.get('age', 'ë¹„ê³µê°œ')}ì„¸
    
     [ì˜¤ëŠ˜ì˜ ìˆ˜ë©´ ë°ì´í„°]
    ìˆ˜ë©´ì‹œê°„: {activity.get('sleep_hours', 'N/A')}ì‹œê°„
    í”¼ë¡œë„ ì ìˆ˜: {activity.get('predicted_fatigue_score', 'N/A')}
    ìˆ˜ë©´ ì§ˆ ì˜ˆì¸¡: {activity.get('predicted_sleep_quality', 'N/A')}
    ì¹´í˜ì¸ ì„­ì·¨: {activity.get('caffeine_mg', 'N/A')}mg
    ì•Œì½”ì˜¬ ì„­ì·¨: {activity.get('alcohol_consumption', 'N/A')}íšŒ
    í™œë™ëŸ‰: {activity.get('physical_activity_hours', 'N/A')}ì‹œê°„
    ì¶”ì²œ ìˆ˜ë©´ì‹œê°„: {activity.get('recommended_sleep_range', 'N/A')}
    
    ---
    ì¶œë ¥ í˜•ì‹:
    1.ìˆ˜ë©´ ìƒíƒœ ìš”ì•½
    2. í”¼ë¡œë„ ë° ì›ì¸ ë¶„ì„
    3. ê°œì„  íŒ 2~3ê°œ
    4. ì§§ì€ ì‘ì› ë©”ì‹œì§€
    """
    
    result = model.generate_content(prompt)
    response = result.text.strip()
    
    save_chat(user_id, "ì¼ê°„ ë¦¬í¬íŠ¸ ìš”ì²­", response, chat_type="report")
    return response

#ì£¼ê°„ ë¦¬í¬íŠ¸
def generate_weekly_report(user_id:int):
    user = get_user_info(user_id) or {}
    week_data = get_weekly_activity(user_id) or {}
    
    if not week_data:
        return "ìµœê·¼ 7ì¼ê°„ì˜ ìˆ˜ë©´ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
    
       # í‰ê·  ê³„ì‚°
    avg_sleep = round(statistics.mean([d["sleep_hours"] for d in week_data if d["sleep_hours"]]), 2)
    avg_fatigue = round(statistics.mean([d["predicted_fatigue_score"] for d in week_data if d["predicted_fatigue_score"]]), 1)
    avg_quality = round(statistics.mean([d["predicted_sleep_quality"] for d in week_data if d["predicted_sleep_quality"]]), 2)
    avg_activity = round(statistics.mean([d["physical_activity_hours"] for d in week_data if d["physical_activity_hours"]]), 1)
    
    prompt = f"""
    ë„ˆëŠ” ì „ë¬¸ ìˆ˜ë©´ ë¶„ì„ ì½”ì¹˜ì•¼.
    ì•„ë˜ì˜ ìµœê·¼ 7ì¼ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì£¼ê°„ ìˆ˜ë©´ ë¦¬í¬íŠ¸'ë¥¼ ë§Œë“¤ì–´ì¤˜.
    
    [ì‚¬ìš©ì]
    ì´ë¦„: {user.get('name', 'ë¹„ê³µê°œ')}
    ë‚˜ì´: {user.get('age', 'ë¹„ê³µê°œ')}ì„¸

    [7ì¼ í‰ê·  ë°ì´í„°]
    í‰ê·  ìˆ˜ë©´ì‹œê°„: {avg_sleep}ì‹œê°„
    í‰ê·  í”¼ë¡œë„ ì ìˆ˜: {avg_fatigue}
    í‰ê·  ìˆ˜ë©´ ì§ˆ ì ìˆ˜: {avg_quality}
    í‰ê·  í™œë™ëŸ‰: {avg_activity}ì‹œê°„
    
    ---
    ì¶œë ¥ í˜•ì‹:
    1. ì£¼ê°„ ìˆ˜ë©´ ìš”ì•½ (ì¢‹ì•˜ë˜ ì , ì•„ì‰¬ìš´ ì )
    2. íŒ¨í„´ ë¶„ì„ (í‰ì¼ vs ì£¼ë§)
    3. ë‹¤ìŒ ì£¼ ê°œì„  íŒ 2ê°€ì§€
    4. ì§§ì€ ë™ê¸°ë¶€ì—¬ ë©”ì‹œì§€
    """
    
    result = model.generate_content(prompt)
    response = result.text.strip()
    
    save_chat(user_id, "ì£¼ê°„ ë¦¬í¬íŠ¸ ìš”ì²­", response, chat_type="report")
    return response



# KANANA
# def generate_sleep_feedback(req):
#     user_id = req.user_id
#     user = get_user_info(user_id) or {}
#     activity = get_daily_activity(user_id) or {}
#     history = get_user_chats(user_id, limit=5) or []

#     history_text = "\n".join(
#         [f"ì‚¬ìš©ì: {h['user_message']} / ì½”ì¹˜: {h['bot_response']}" for h in history]
#     ) or "ìµœê·¼ ëŒ€í™” ì—†ìŒ"

#     user_info = "\n".join(filter(None, [
#         _fmt("ì´ë¦„", user.get("name")),
#         _fmt("ë‚˜ì´", user.get("age"), "ì„¸"),
#         _fmt("ì„±ë³„", user.get("gender")),
#     ]))

#     activity_info = "\n".join(filter(None, [
#         _fmt("ìˆ˜ë©´ì‹œê°„", activity.get("sleep_hours"), "ì‹œê°„"),
#         _fmt("í”¼ë¡œë„ ì ìˆ˜", activity.get("fatigue_score")),
#         _fmt("ì¹´í˜ì¸ ì„­ì·¨ëŸ‰", activity.get("caffeine_mg"), "mg"),
#         _fmt("ì•Œì½”ì˜¬ ì„­ì·¨ëŸ‰", activity.get("alcohol_consumption"), "íšŒ"),
#         _fmt("í™œë™ëŸ‰", activity.get("physical_activity_hours"), "ì‹œê°„"),
#         _fmt("ì¶”ì²œ ìˆ˜ë©´ ì‹œê°„", activity.get("recommended_range")),
#     ]))

#     #í•µì‹¬: 'ì˜ˆì‹œ ì¶œë ¥' 'ê·œì¹™' ë“± ì œê±°í•˜ê³ , ë‹µë³€ ì‹œì‘ ì§€ì  ëª…í™•íˆ ì§€ì •
#     prompt = f"""
#         ë„ˆëŠ” ë”°ëœ»í•˜ê³  ì¹œì ˆí•œ ìˆ˜ë©´ ì½”ì¹˜ì•¼.
#         ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°„ê²°í•˜ê³  ë”°ëœ»í•œ í”¼ë“œë°±ì„ ì¤˜.

#         [ì‚¬ìš©ì ì •ë³´]
#         {user_info or '- ì •ë³´ ì—†ìŒ'}

#         [ìµœê·¼ í•˜ë£¨ ë°ì´í„°]
#         {activity_info or '- ë°ì´í„° ì—†ìŒ'}

#         [ìµœê·¼ ëŒ€í™”]
#         {history_text}

#         [ì‚¬ìš©ì ì§ˆë¬¸]
#         {req.message}

#         ---

#         ë‹¤ìŒ ë‚´ìš©ì„ ë°˜ë“œì‹œ í¬í•¨í•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´ì¤˜:
#         1. ìƒíƒœ ìš”ì•½ (í˜„ì¬ ìˆ˜ë©´ ìƒíƒœ í•œ ì¤„)
#         2. ì˜¤ëŠ˜ì˜ ì»¨ë””ì…˜ ì½”ë©˜íŠ¸
#         3. ì‹¤ì²œ ê°€ëŠ¥í•œ ìˆ˜ë©´ ê°œì„  íŒ 2~3ê°œ (ë¶ˆë¦¿ í˜•ì‹)
#         4. ë§ˆì§€ë§‰ì— ë”°ëœ»í•œ ì‘ì›ì˜ ë§

#         ì‘ë‹µì€ ì•„ë˜ í˜•ì‹ì„ ì°¸ê³ í•˜ë˜, ì˜ˆì‹œëŠ” ì¶œë ¥í•˜ì§€ ë§ˆ.
#         ### ë‹µë³€ ì‹œì‘:
#         """

#     result = generator(
#         prompt,
#         max_new_tokens=250,
#         do_sample=True,
#         temperature=0.7,
#         top_p=0.9,
#         repetition_penalty=1.1,
#         return_full_text=False,
#         eos_token_id=TOKENIZER.eos_token_id,
#     )

#     response = result[0]["generated_text"].strip()

#     save_chat(user_id, req.message, response, chat_type="sleep")
#     return response
    