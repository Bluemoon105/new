from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from config import settings
from services.db_service import get_user_info, get_daily_activity
from services.mongo_service import save_chat, get_user_chats

MODEL_NAME = settings.KANANA_MODEL
TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME, token=settings.HF_TOKEN)
MODEL = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype="auto",
    device_map="auto",
    token=settings.HF_TOKEN,
)
generator = pipeline("text-generation", model=MODEL, tokenizer=TOKENIZER, device_map="auto")

# -------------------------
# ìœ í‹¸: ë¹ˆ ê°’ ì œê±°
# -------------------------
def _fmt(label: str, value, unit: str = ""):
    if not value or value in ["N/A", "None", "ë¹„ê³µê°œ"]:
        return ""
    return f"- {label}: {value}{unit}"


def generate_sleep_feedback(req):
    user_id = req.user_id
    user = get_user_info(user_id) or {}
    activity = get_daily_activity(user_id) or {}
    history = get_user_chats(user_id, limit=5) or []

    history_text = "\n".join(
        [f"ì‚¬ìš©ì: {h['user_message']} / ì½”ì¹˜: {h['bot_response']}" for h in history]
    ) or "ìµœê·¼ ëŒ€í™” ì—†ìŒ"

    user_info = "\n".join(filter(None, [
        _fmt("ì´ë¦„", user.get("name")),
        _fmt("ë‚˜ì´", user.get("age"), "ì„¸"),
        _fmt("ì„±ë³„", user.get("gender")),
    ]))

    activity_info = "\n".join(filter(None, [
        _fmt("ìˆ˜ë©´ì‹œê°„", activity.get("sleep_hours"), "ì‹œê°„"),
        _fmt("í”¼ë¡œë„ ì ìˆ˜", activity.get("fatigue_score")),
        _fmt("ì¹´í˜ì¸ ì„­ì·¨ëŸ‰", activity.get("caffeine_mg"), "mg"),
        _fmt("ì•Œì½”ì˜¬ ì„­ì·¨ëŸ‰", activity.get("alcohol_consumption"), "íšŒ"),
        _fmt("í™œë™ëŸ‰", activity.get("physical_activity_hours"), "ì‹œê°„"),
        _fmt("ì¶”ì²œ ìˆ˜ë©´ ì‹œê°„", activity.get("recommended_range")),
    ]))

    # --------------------------------------
    # ğŸš€ í•µì‹¬: 'ì˜ˆì‹œ ì¶œë ¥' 'ê·œì¹™' ë“± ì œê±°í•˜ê³ , ë‹µë³€ ì‹œì‘ ì§€ì  ëª…í™•íˆ ì§€ì •
    # --------------------------------------
    prompt = f"""
ë„ˆëŠ” ë”°ëœ»í•˜ê³  ì¹œì ˆí•œ ìˆ˜ë©´ ì½”ì¹˜ì•¼.
ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°„ê²°í•˜ê³  ë”°ëœ»í•œ í”¼ë“œë°±ì„ ì¤˜.

[ì‚¬ìš©ì ì •ë³´]
{user_info or '- ì •ë³´ ì—†ìŒ'}

[ìµœê·¼ í•˜ë£¨ ë°ì´í„°]
{activity_info or '- ë°ì´í„° ì—†ìŒ'}

[ìµœê·¼ ëŒ€í™”]
{history_text}

[ì‚¬ìš©ì ì§ˆë¬¸]
{req.message}

---

ë‹¤ìŒ ë‚´ìš©ì„ ë°˜ë“œì‹œ í¬í•¨í•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´ì¤˜:
1. ìƒíƒœ ìš”ì•½ (í˜„ì¬ ìˆ˜ë©´ ìƒíƒœ í•œ ì¤„)
2. ì˜¤ëŠ˜ì˜ ì»¨ë””ì…˜ ì½”ë©˜íŠ¸
3. ì‹¤ì²œ ê°€ëŠ¥í•œ ìˆ˜ë©´ ê°œì„  íŒ 2~3ê°œ (ë¶ˆë¦¿ í˜•ì‹)
4. ë§ˆì§€ë§‰ì— ë”°ëœ»í•œ ì‘ì›ì˜ ë§

ì‘ë‹µì€ ì•„ë˜ í˜•ì‹ì„ ì°¸ê³ í•˜ë˜, ì˜ˆì‹œëŠ” ì¶œë ¥í•˜ì§€ ë§ˆ.
### ë‹µë³€ ì‹œì‘:
"""

    result = generator(
        prompt,
        max_new_tokens=250,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1,
        return_full_text=False,
        eos_token_id=TOKENIZER.eos_token_id,
    )

    response = result[0]["generated_text"].strip()

    save_chat(user_id, req.message, response, chat_type="sleep")
    return response
